\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{fancybox}
\usepackage{titlesec}
\usepackage{color}

% Margins
\geometry{margin=1in}

% Theorem-like environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]

% For administrative notes
\newenvironment{administrative_note}
  {\vspace{1em}\noindent\setlength{\fboxrule}{1pt}\begin{Sbox}\begin{minipage}{0.98\textwidth}\color{blue}\sffamily\large}
  {\end{minipage}\end{Sbox}\fbox{\TheSbox}\vspace{1em}}

% Section formatting for elegance
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{0.75em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}{\normalfont\bfseries}{\thesubsubsection}{0.5em}{}

% Document
\begin{document}

\begin{center}
    {\LARGE \textbf{Tutorial 3 — Solutions and Conceptual Guide}} \\[1ex]
    {\large Undergraduate Mathematics \\ [0.5ex] \hrule}
\end{center}

\vspace{1em}

%======================
% ADMINISTRATIVE NOTES
%======================

\begin{administrative_note}
\textbf{Course Announcements for Tutorial 3}
\begin{itemize}
    \item \textbf{Submission Deadline:} Homework solutions for Tutorial 3 are due \textbf{Friday, March 12, at 11:59 PM} via the course LMS.
    \item \textbf{Office Hours:} Prof. Lee will hold extended office hours on \textbf{Wednesday, March 10, from 3:00-5:00 PM}.
    \item \textbf{Midterm Reminder:} The midterm exam will take place \textbf{next Monday (March 15)} in Lecture Hall B, 9:00–10:30 AM. Please bring your student ID.
    \item \textbf{Correction:} In Question 2(b) below, an error was found in the original answer key (an incorrect sign in the final calculation). The corrected solution is given here.
\end{itemize}
\end{administrative_note}

%======================
% MATHEMATICAL CONTENT
%======================

\section{Motivation and Overview}

The purpose of this tutorial is to deepen your understanding of linear algebraic concepts, with a particular focus on linear transformations, their representation by matrices, and the computation of kernels and images. As you work through these problems, strive not only to master calculation, but also to appreciate the elegant structure underlying vector spaces and linear maps.

Let us remember: Linear transformations are fundamental because they preserve the structure of vector spaces—they capture the essence of "linearity," which is at the heart of much mathematics, physics, and engineering.

\section{Worked Problems and Guided Solutions}

\subsection{Problem 1: The Kernel and Image of a Matrix}

Let \( A = 
\begin{pmatrix}
2 & 4 & 1 \\
0 & 0 & 0 \\
1 & 2 & 1
\end{pmatrix} \).

We are to find:
\begin{enumerate}[label=(\alph*)]
    \item The kernel (also known as the null space) of \( A \), i.e., all vectors \( \vec{x} \in \mathbb{R}^3 \) such that \( A\vec{x} = \vec{0} \).
    \item The image (also known as the column space or range) of \( A \).
\end{enumerate}

\subsubsection*{Motivation}

Understanding the kernel and image of a linear transformation tells us about its invertibility, solutions to systems, and the structure of subspaces in \( \mathbb{R}^n \).

\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
% (a)
\item \textbf{The Kernel of \( A \)}.

We solve for all \( \vec{x} = \begin{pmatrix}x_1 \\ x_2 \\ x_3\end{pmatrix} \) such that \( A\vec{x} = \vec{0} \):

\[
A\vec{x} = 
\begin{pmatrix}
2 & 4 & 1 \\
0 & 0 & 0 \\
1 & 2 & 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
=
\begin{pmatrix}
2x_1 + 4x_2 + x_3 \\
0 \\
x_1 + 2x_2 + x_3
\end{pmatrix}
= 
\begin{pmatrix}
0 \\ 0 \\ 0
\end{pmatrix}
\]

Let us write out the system:
\[
\begin{cases}
2x_1 + 4x_2 + x_3 = 0 \\
0 = 0 \\
x_1 + 2x_2 + x_3 = 0 
\end{cases}
\]

The second equation gives no information. Let's work with the other two:

Let us subtract the second equation from the first:
\[
(2x_1 + 4x_2 + x_3) - 2(x_1 + 2x_2 + x_3) = 0 - 2 \cdot 0
\]
But this is unnecessary; in fact, notice that the first equation is twice the third equation, except for the \( x_3 \) coefficient:

Let us solve the third equation for \( x_1 \):
\[
x_1 + 2x_2 + x_3 = 0 \implies x_1 = -2x_2 - x_3
\]

Plug this \( x_1 \) into the first equation:
\begin{align*}
2x_1 + 4x_2 + x_3 &= 0\\
2(-2x_2 - x_3) + 4x_2 + x_3 &= 0\\
(-4x_2 - 2x_3) + 4x_2 + x_3 &= 0 \\
(0x_2) + (-2x_3 + x_3) &= 0\\
(-x_3) = 0 \implies x_3 = 0
\end{align*}

Now, plugging \( x_3 = 0 \) back into the equation for \( x_1 \):
\[
x_1 = -2x_2 - 0 = -2x_2
\]

So, the solution set is:
\[
\vec{x} = 
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
=
\begin{pmatrix}
-2x_2 \\ x_2 \\ 0
\end{pmatrix}
= 
x_2 \begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix}
\]
where \( x_2 \in \mathbb{R} \).

\begin{center}
    \fbox{
        \parbox{0.93\textwidth}{
        \textbf{Conclusion:} \\
        The kernel of \( A \) is the set
        \[
        \mathrm{Ker}(A) = \left\{ \lambda \begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix} : \lambda \in \mathbb{R} \right\}
        \]
        i.e., it is a \emph{one-dimensional subspace} of \( \mathbb{R}^3 \) spanned by \( (-2,\ 1,\ 0)^\top \).
        }
    }
\end{center}

\begin{remark}
    This kernel consists of all vectors in \( \mathbb{R}^3 \) that are ``annihilated'' by \( A \); that is, they are precisely those inputs which are sent to the zero vector under this transformation.
\end{remark}

% (b)
\item \textbf{The Image of \( A \)}.

Recall that the image of \( A \) is the set of all vectors in \( \mathbb{R}^3 \) which can be written as \( A\vec{x} \) for some \( \vec{x} \).

Alternatively, the image is the span of the columns of \( A \):

\[
A = 
\begin{pmatrix}
\mathbf{2} & \mathbf{4} & \mathbf{1} \\
0 & 0 & 0 \\
1 & 2 & 1
\end{pmatrix}
= \left[ \begin{array}{ccc}
\vec{a}_1 & \vec{a}_2 & \vec{a}_3
\end{array} \right]
\]
with \(
\vec{a}_1 = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix},
\vec{a}_2 = \begin{pmatrix} 4 \\ 0 \\ 2 \end{pmatrix},
\vec{a}_3 = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}
\).

Let us ask: are these columns linearly independent? Are any redundant?

Let us check for dependencies.

Observe that:
\[
\vec{a}_2 = 2\vec{a}_1, \quad \vec{a}_3 = \vec{a}_1 - \vec{a}_1 + \vec{a}_3
\]

Let us check if \( \vec{a}_3 \) can be written as a linear combination of \( \vec{a}_1 \) and \( \vec{a}_2 \):

Suppose \( \vec{a}_3 = \alpha \vec{a}_1 + \beta \vec{a}_2 \).
Then,
\[
\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} = \alpha \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} + \beta \begin{pmatrix} 4 \\ 0 \\ 2 \end{pmatrix}
= \begin{pmatrix} 2\alpha + 4\beta \\ 0 \\ \alpha + 2\beta \end{pmatrix}
\]

So, we have the system:
\[
\begin{cases}
2\alpha + 4\beta = 1 \\
\alpha + 2\beta = 1
\end{cases}
\]

Solving the second equation for \( \alpha \):
\(
\alpha = 1 - 2\beta
\)

Plug into the first equation:
\[
2(1 - 2\beta) + 4\beta = 1 \\
2 - 4\beta + 4\beta = 1 \\
2 = 1
\]

This is a contradiction.

Therefore, \( \vec{a}_3 \) \emph{cannot} be written as a linear combination of \( \vec{a}_1 \) and \( \vec{a}_2 \).

But notice that \( \vec{a}_2 = 2\vec{a}_1 \), so \( \vec{a}_2 \) is redundant.

Thus, the image is the span of \( \{ \vec{a}_1, \vec{a}_3 \} \), which are linearly independent.

Let us check their independence:

Suppose \( \lambda_1 \vec{a}_1 + \lambda_3 \vec{a}_3 = \vec{0} \):

\[
\lambda_1 \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix}
+ \lambda_3 \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
\]
So:
\[
2\lambda_1 + \lambda_3 = 0 \\
0 = 0 \\
\lambda_1 + \lambda_3 = 0
\]
The second equation is superfluous.

From the third: \( \lambda_1 + \lambda_3 = 0 \implies \lambda_3 = -\lambda_1 \).

Plug into the first: \( 2\lambda_1 - \lambda_1 = 0 \implies \lambda_1 = 0 \implies \lambda_3=0 \).

Thus, \( \vec{a}_1 \) and \( \vec{a}_3 \) are indeed linearly independent.

\begin{center}
    \fbox{
        \parbox{0.93\textwidth}{
        \textbf{Conclusion:} \\
        The image of \( A \) is the subspace of \( \mathbb{R}^3 \) spanned by the vectors
        \[
        \vec{a}_1 = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix}, \qquad
        \vec{a}_3 = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}
        \]
        That is,
        \[
        \mathrm{Im}(A) = \operatorname{span} \left\{ \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix},\ \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} \right\}
        \]
        This is a \emph{two-dimensional subspace} of \( \mathbb{R}^3 \) consisting of all vectors of the form \(
        \alpha \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} + \beta \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}
        \).
        }
    }
\end{center}

\begin{remark}
    In summary, this matrix \( A \) has a one-dimensional kernel and a two-dimensional image; together, these dimensions add up to 3, the number of columns, as anticipated by the Rank-Nullity Theorem.
\end{remark}

\end{enumerate}

\begin{example}[Enhancement: Non-example]
    Consider \( B = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix} \). Here, the kernel is all \( \vec{x} = \begin{pmatrix}x_1 \\ x_2\end{pmatrix} \) such that \( x_1 = 0, x_2=0 \), so the only solution is the zero vector; the kernel is trivial. The image is the $xy$-plane in \( \mathbb{R}^3 \), so it is two-dimensional.
\end{example}

\begin{remark}[On the Rank-Nullity Theorem]
    Recall: For any linear transformation \( T: V \to W \) (with finite-dimensional domain), \[
    \dim(\mathrm{Ker}(T)) + \dim(\mathrm{Im}(T)) = \dim(V).
    \]
    For the matrix \( A \) above: \( 1 + 2 = 3 \), confirming the theorem.
\end{remark}

\subsection{Problem 2: Solving a System of Linear Equations}

Consider the following system:
\[
\begin{cases}
x + 2y + z = 1 \\
2x + 3y + 2z = 2 \\
3x + 6y + 3z = 3
\end{cases}
\]

Let us:

\begin{enumerate}[label=(\alph*)]
    \item Write the augmented matrix and use row reduction to solve the system.
    \item Determine whether the solution set is a point, a line, a plane, or empty.
\end{enumerate}

\subsubsection*{Motivation}

This exercise illustrates how the structure of the solution set to a system reflects the dependencies among the equations (which themselves correspond to the geometric intersection of planes in space).

\subsubsection*{Solution}

\begin{enumerate}[label=(\alph*)]
% (a)
\item \textbf{Augmented Matrix and Row Reduction}

First, write the augmented matrix:
\[
\left(
\begin{array}{ccc|c}
1 & 2 & 1 & 1 \\
2 & 3 & 2 & 2 \\
3 & 6 & 3 & 3
\end{array}
\right)
\]

Let's proceed by standard Gaussian elimination:

\begin{itemize}
    \item \textbf{Step 1 (First Pivot):} The first row is unchanged.

    \item \textbf{Eliminate below:}

    Row 2: \( R_2 - 2 \times R_1 \):
    \[
    (2, 3, 2\,|\,2) - 2 \times (1, 2, 1\,|\,1) = (0, -1, 0\,|\,0)
    \]

    Row 3: \( R_3 - 3 \times R_1 \):
    \[
    (3, 6, 3\,|\,3) - 3 \times (1, 2, 1\,|\,1) = (0, 0, 0\,|\,0)
    \]

    Updated matrix:
    \[
    \left(
    \begin{array}{ccc|c}
    1 & 2 & 1 & 1 \\
    0 & -1 & 0 & 0 \\
    0 & 0 & 0 & 0
    \end{array}
    \right)
    \]

    \item \textbf{Step 2 (Second Pivot):}
    Multiply row 2 by \(-1\) to get a leading 1:

    \[
    (0, -1, 0\,|\,0) \to (0, 1, 0\,|\,0)
    \]

    Updated matrix:
    \[
    \left(
    \begin{array}{ccc|c}
    1 & 2 & 1 & 1 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0
    \end{array}
    \right)
    \]

    \item \textbf{Back-substitute:} 

    From row 2: \( y = 0 \).

    From row 1: \( x + 2y + z = 1 \implies x + z = 1 \implies x = 1 - z \).

    \( z \) is a free variable.
\end{itemize}

Thus, the general solution is:
\[
\boxed{
\begin{aligned}
x &= 1 - z \\
y &= 0 \\
z &= z \text{ (free)}
\end{aligned}
}
\]


Therefore, the complete solution set is
\[
\left\{
\begin{pmatrix}
1 - t \\
0 \\
t
\end{pmatrix}
: t \in \mathbb{R}
\right\}
\]
Alternatively,
\[
\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}
+ t \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix}, \quad t \in \mathbb{R}
\]

\begin{example}[Original Example]
    Let \( t = 0 \): \( (x, y, z) = (1, 0, 0) \).

    If \( t = 2 \): \( (x, y, z) = (1-2, 0, 2) = (-1, 0, 2) \).

    Both are solutions to the system.
\end{example}

% (b)
\item \textbf{Geometric Nature of the Solution Set}

Notice that there is one free parameter (\( z \)), while \( x \) and \( y \) are determined. Thus, the solution set forms a \emph{line} in \( \mathbb{R}^3 \).

\vspace{0.5em}
\noindent\textbf{Conclusion:} \emph{The solution set is a line} (i.e., a one-dimensional affine subspace of \( \mathbb{R}^3 \)), described by
\[
\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}
+ t \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix}, \quad t \in \mathbb{R}
\]
\end{enumerate}

\begin{remark}
    The infinite nature of solutions, in the form of a line, arises from the \emph{linear dependence} among the equations: Here, the third equation is just three times the first, so it adds no new information.
\end{remark}

\subsection{Problem 3: Proof Sketch — Images and Kernels Under Composition}

Let \( T: V \to W \) and \( S: W \to U \) be linear transformations between vector spaces.

Prove or disprove:

\begin{quote}
    ``The kernel of the composition \( S \circ T \) is the preimage under \( T \) of the kernel of \( S \): \(
    \mathrm{Ker}(S \circ T) = T^{-1}(\mathrm{Ker}\,S)
    \).''
\end{quote}

\subsubsection*{Motivation}

This is about understanding how linear maps interact under composition, and how information is ``filtered'' through successive transformations.

\subsubsection*{Solution}

By definition,
\[
\mathrm{Ker}(S \circ T) = \{ v \in V : S(T(v)) = 0 \}
\]

Now, \( T^{-1}(\mathrm{Ker}\,S) = \{ v \in V : T(v) \in \mathrm{Ker}\,S \} \).

But \( S(T(v)) = 0 \) if and only if \( T(v) \in \mathrm{Ker}\,S \).

So the two sets are equal!

\[
\boxed{
\mathrm{Ker}(S \circ T) = T^{-1}(\mathrm{Ker}\,S)
}
\]

\begin{proof}[Proof]
Let \( v \in V \).
\begin{align*}
v \in \mathrm{Ker}(S \circ T) 
&\iff S(T(v)) = 0_{U} \\
&\iff T(v) \in \mathrm{Ker}\,S \\
&\iff v \in T^{-1}(\mathrm{Ker}\,S)
\end{align*}
Therefore, the equality holds.
\end{proof}

\begin{example}[Original Example]
    Suppose \( T: \mathbb{R}^2 \to \mathbb{R}^2 \) defined by \( T\left( \begin{pmatrix} x \\ y \end{pmatrix} \right) = \begin{pmatrix} x + y \\ y \end{pmatrix} \), and \( S: \mathbb{R}^2 \to \mathbb{R} \), \( S\left( \begin{pmatrix} u \\ v \end{pmatrix} \right) = u \).

    Find \( \mathrm{Ker}(S \circ T) \):

    \( S(T(x,y)) = S(x+y, y) = x+y \).
    
    So \( S(T(x,y)) = 0 \iff x + y = 0 \implies x = -y \).
    
    Thus,
    \[
    \mathrm{Ker}(S \circ T) = \left\{ \begin{pmatrix} -y \\ y \end{pmatrix} : y \in \mathbb{R} \right\}
    \]
    
    Now, \( \mathrm{Ker}(S) = \left\{ \begin{pmatrix} 0 \\ v \end{pmatrix} : v \in \mathbb{R} \right\} \).

    \( T(x,y) \in \mathrm{Ker}(S) \implies x+y=0 \implies x = -y \), as before.

    Thus, both calculations give the same set.
\end{example}

\section*{Summary and Takeaways}

\begin{itemize}
    \item The kernel and image of a matrix are fundamental concepts that reveal the ``action'' of a linear transformation.
    \item Row reduction provides both a computational technique and geometric insight into the solutions to systems of linear equations.
    \item The behavior of compositions of linear maps, especially regarding their kernels, formalizes our intuitive expectation about ``filtering'' information through several maps.
    \item Always link computational work to geometric and theoretical understanding!
\end{itemize}

\begin{administrative_note}
\textbf{Reminder:} Tutorial 4 problems will be posted by this Friday. Please consult the course forum for clarifications or to discuss concepts with your classmates.
\end{administrative_note}

\vspace{3em}
\begin{center}
    \Large \textit{End of Tutorial 3 Solutions}
\end{center}

\end{document}
