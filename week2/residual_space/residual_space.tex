\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{a4paper, margin=1in} % Adjust margins for better readability
\usepackage{hyperref} % Optional: for clickable links if needed

% --- Custom Commands ---
\newcommand{\mat}[1]{\mathbf{#1}} % Matrix notation
\newcommand{\vecspace}[1]{\mathcal{#1}} % Vector space notation
\newcommand{\colspace}[1]{\vecspace{C}(#1)} % Column space
\newcommand{\nullspace}[1]{\vecspace{N}(#1)} % Null space
\newcommand{\transpose}[1]{#1^{\top}} % Transpose
\newcommand{\proj}[1]{\mat{P}_{#1}} % Projection matrix onto space #1
\newcommand{\identity}{\mat{I}} % Identity matrix
\newcommand{\realnums}{\mathbb{R}} % Real numbers

% --- Theorem Styles ---
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\title{Understanding the Residual Space: Orthogonality in Projections}
\author{Your Friendly Neighborhood Math Teacher}
\date{\today}

\begin{document}
\maketitle

\section{Motivation: Decomposing Vectors}

In many areas of mathematics, science, and engineering, particularly in statistics and data analysis (think linear regression!), we are interested in understanding how a vector $\vec{y}$ relates to a specific subspace. Often, this subspace is the \textbf{column space} of a matrix $\mat{X}$, denoted $\colspace{\mat{X}}$. This space represents the set of all possible outcomes or predictions generated by a linear model defined by $\mat{X}$.

A fundamental idea is to decompose $\vec{y}$ into two components:
\begin{enumerate}
    \item A component that lies *within* the subspace $\colspace{\mat{X}}$.
    \item A component that is *orthogonal* (perpendicular) to the subspace $\colspace{\mat{X}}$.
\end{enumerate}
This second component, the part "left over" after accounting for the column space, is what we call the \textbf{residual}. The space containing all such residuals is the \textbf{residual space}. Let's explore this formally.

\section{Projections and Orthogonality}

\begin{definition}[Column Space]
Let $\mat{X}$ be an $m \times n$ matrix. The \textbf{column space} of $\mat{X}$, denoted $\colspace{\mat{X}}$, is the subspace of $\realnums^m$ spanned by the column vectors of $\mat{X}$.
\[ \colspace{\mat{X}} = \{ \mat{X}\vec{a} \mid \vec{a} \in \realnums^n \} \]
\end{definition}

\begin{definition}[Orthogonal Projection Matrix]
Let $\vecspace{W}$ be a subspace of $\realnums^m$. The \textbf{orthogonal projection matrix} onto $\vecspace{W}$, denoted $\proj{\vecspace{W}}$, is the unique matrix such that for any vector $\vec{y} \in \realnums^m$:
\begin{itemize}
    \item $\proj{\vecspace{W}}\vec{y} \in \vecspace{W}$ (The projection lies in the subspace).
    \item $\vec{y} - \proj{\vecspace{W}}\vec{y}$ is orthogonal to every vector in $\vecspace{W}$.
\end{itemize}
If the columns of $\mat{X}$ form a basis for $\vecspace{W}$, then $\proj{\vecspace{W}} = \mat{X}(\transpose{\mat{X}}\mat{X})^{-1}\transpose{\mat{X}}$. We often denote the projection onto the column space of $\mat{X}$ simply as $\proj{\mat{X}}$.
\end{definition}

\begin{remark}
The vector $\proj{\mat{X}}\vec{y}$ is the vector in $\colspace{\mat{X}}$ that is "closest" to $\vec{y}$ in the Euclidean distance sense. In linear regression, if $\vec{y}$ represents observed data and $\colspace{\mat{X}}$ represents possible model predictions, then $\proj{\mat{X}}\vec{y}$ gives the best-fit predictions.
\end{remark}

Now, consider the identity matrix $\identity$. It represents the projection onto the entire space $\realnums^m$ (i.e., $\identity \vec{y} = \vec{y}$ for all $\vec{y}$).

\section{The Residual Space}

What happens when we consider the matrix $\identity - \proj{\mat{X}}$?

\begin{proposition}
The matrix $\mat{M} = \identity - \proj{\mat{X}}$ is also an orthogonal projection matrix. It projects vectors onto the \textbf{orthogonal complement} of the column space, denoted $\colspace{\mat{X}}^{\perp}$.
\[ \colspace{\mat{X}}^{\perp} = \{ \vec{z} \in \realnums^m \mid \transpose{\vec{z}}\vec{w} = 0 \text{ for all } \vec{w} \in \colspace{\mat{X}} \} \]
\end{proposition}

\begin{proof}[Sketch of Proof]
We need to show that $\mat{M}\vec{y}$ lies in $\colspace{\mat{X}}^{\perp}$ and that $\vec{y} - \mat{M}\vec{y}$ is orthogonal to $\colspace{\mat{X}}^{\perp}$.
\begin{itemize}
    \item $\mat{M}\vec{y} = (\identity - \proj{\mat{X}})\vec{y} = \vec{y} - \proj{\mat{X}}\vec{y}$. By definition of $\proj{\mat{X}}$, this vector is orthogonal to $\colspace{\mat{X}}$, hence it lies in $\colspace{\mat{X}}^{\perp}$.
    \item $\vec{y} - \mat{M}\vec{y} = \vec{y} - (\vec{y} - \proj{\mat{X}}\vec{y}) = \proj{\mat{X}}\vec{y}$. This vector lies in $\colspace{\mat{X}}$. Since any vector in $\realnums^m$ can be uniquely decomposed into a part in $\colspace{\mat{X}}$ and a part in $\colspace{\mat{X}}^{\perp}$, and these two spaces are orthogonal, $\proj{\mat{X}}\vec{y}$ must be orthogonal to $\colspace{\mat{X}}^{\perp}$.
\end{itemize}
Furthermore, projection matrices are idempotent ($\mat{P}^2 = \mat{P}$) and symmetric ($\transpose{\mat{P}} = \mat{P}$). One can verify that $\mat{M}$ also has these properties.
\end{proof}

\begin{definition}[Residual Space]
The \textbf{residual space} associated with the matrix $\mat{X}$ (or more accurately, with the projection $\proj{\mat{X}}$) is the orthogonal complement of the column space of $\mat{X}$, i.e., $\colspace{\mat{X}}^{\perp}$. It is the subspace onto which the matrix $(\identity - \proj{\mat{X}})$ projects. Any vector in this space is called a residual vector.
\end{definition}

\begin{remark}
The image you provided highlights the crucial property: $\transpose{\mat{X}}(\identity - \proj{\mat{X}}) = \mat{0}$. Let's see why.
Any column of $(\identity - \proj{\mat{X}})$ is the result of applying this projection to a standard basis vector, say $\vec{e}_j$. The result, $(\identity - \proj{\mat{X}})\vec{e}_j$, lies in the residual space $\colspace{\mat{X}}^{\perp}$.
The rows of $\transpose{\mat{X}}$ are the columns of $\mat{X}$, which span $\colspace{\mat{X}}$.
The $(i, j)$-th entry of the product $\transpose{\mat{X}}(\identity - \proj{\mat{X}})$ is the dot product of the $i$-th row of $\transpose{\mat{X}}$ (which is the $i$-th column of $\mat{X}$, a vector in $\colspace{\mat{X}}$) and the $j$-th column of $(\identity - \proj{\mat{X}})$ (a vector in $\colspace{\mat{X}}^{\perp}$).
Since these two spaces are orthogonal by definition, their dot product must be zero. As this holds for all $i$ and $j$, the resulting matrix is the zero matrix $\mat{0}$. This equation mathematically confirms that the residual space is orthogonal to the column space.
\end{remark}

\section{Geometric Intuition}

Imagine $\realnums^3$. Let $\colspace{\mat{X}}$ be a plane through the origin (a 2-dimensional subspace).
\begin{itemize}
    \item Pick any vector $\vec{y}$ in $\realnums^3$.
    \item $\proj{\mat{X}}\vec{y}$ is the "shadow" of $\vec{y}$ cast perpendicularly onto the plane $\colspace{\mat{X}}$.
    \item $(\identity - \proj{\mat{X}})\vec{y}$ is the vector connecting the shadow $\proj{\mat{X}}\vec{y}$ to the original vector $\vec{y}$. This vector is perpendicular (orthogonal) to the plane $\colspace{\mat{X}}$. It is the residual.
    \item The \textbf{residual space}, $\colspace{\mat{X}}^{\perp}$, is the line passing through the origin that is perpendicular to the plane $\colspace{\mat{X}}$. All possible residual vectors lie along this line.
\end{itemize}
The projection $(\identity - \proj{\mat{X}})$ takes any vector $\vec{y}$ and finds its component along this perpendicular line (the residual space).

This decomposition is fundamental in understanding linear models, allowing us to separate the part of the data explained by the model (the projection onto $\colspace{\mat{X}}$) from the unexplained part (the projection onto the residual space $\colspace{\mat{X}}^{\perp}$).

\end{document}
