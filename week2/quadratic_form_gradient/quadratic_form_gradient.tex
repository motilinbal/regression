\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{palatino} % Using Palatino font for a slightly more elegant look
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=blue]{hyperref}

\geometry{a4paper, margin=1in}

% --- Custom Theorem Styles ---
\newtheoremstyle{mytheoremstyle}
  {\topsep} % Space above
  {\topsep} % Space below
  {\itshape} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec (can be left empty, meaning `normal')

\newtheoremstyle{mydefinitionstyle}
  {\topsep} % Space above
  {\topsep} % Space below
  {} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec

\theoremstyle{mydefinitionstyle}
\newtheorem{definition}{Definition}[section]
\theoremstyle{mytheoremstyle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

% --- Math Macros ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}} % Using a script V for the vector space
\newcommand{\T}{\mathbf{T}} % Bold T for the operator
\newcommand{\Tadj}{\mathbf{T}^*} % Adjoint operator
\newcommand{\x}{\mathbf{x}} % Bold x for vectors
\newcommand{\h}{\mathbf{h}} % Bold h for displacement vectors
\newcommand{\uvec}{\mathbf{u}} % Bold u
\newcommand{\vvec}{\mathbf{v}} % Bold v
\newcommand{\grad}{\nabla} % Gradient symbol
\newcommand{\inner}[2]{\langle #1, #2 \rangle} % Inner product
\newcommand{\ddt}{\frac{d}{dt}}
\newcommand{\pdx}[1]{\frac{\partial #1}{\partial \x}} % Partial derivative w.r.t vector x

\title{The Gradient of a Quadratic Form: An Operator Perspective}
\author{Your Friendly Math Teacher}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This note presents a coordinate-free derivation of the gradient of a general quadratic form $f(\x) = \inner{\x}{\T\x}$ defined on a real inner product space. We emphasize the roles of the directional derivative, the inner product structure, and the adjoint operator.
\end{abstract}

\section{Introduction: Beyond Matrices}

Quadratic forms appear everywhere â€“ from conic sections and optimization problems in calculus to energy functionals in physics and variance calculations in statistics. Often, we encounter them in the matrix form $f(\x) = \x^T A \x$. Finding the gradient, $\pdx{f}$, is a standard calculation, resulting in $(A + A^T)\x$, or $2A\x$ if $A$ is symmetric.

While the matrix approach is effective, especially for computation, viewing the problem through the lens of linear operators on abstract vector spaces offers deeper insights. It strips away the dependence on a specific basis and highlights the fundamental geometric and algebraic structures involved. Our goal today is to derive the gradient of $f(\x) = \inner{\x}{\T\x}$ using this more general framework.

\section{Setting the Stage}

Let's clearly define our playground:
\begin{definition}[Inner Product Space]
    We work within a finite-dimensional real vector space $\V$, equipped with an inner product $\inner{\cdot}{\cdot}: \V \times \V \to \R$. Recall that an inner product is a symmetric, bilinear, positive-definite form. It provides notions of length, angle, and orthogonality.
\end{definition}

\begin{definition}[Linear Operator]
    Let $\T: \V \to \V$ be a linear operator. Linearity means $\T(a\uvec + b\vvec) = a\T\uvec + b\T\vvec$ for all $\uvec, \vvec \in \V$ and $a, b \in \R$.
\end{definition}

\begin{definition}[Quadratic Form]
    The function we wish to differentiate is the quadratic form $f: \V \to \R$ associated with $\T$, defined by:
    \[ f(\x) = \inner{\x}{\T\x} \]
\end{definition}

\section{The Gradient and the Directional Derivative}

How do we define the "gradient" in this abstract setting? We use the concept of the directional derivative.

\begin{definition}[Directional Derivative]
    The directional derivative of $f$ at $\x$ in the direction $\h \in \V$ measures the instantaneous rate of change of $f$ as we move from $\x$ along $\h$. It's defined as:
    \[ D_{\h}f(\x) = \lim_{t \to 0} \frac{f(\x + t\h) - f(\x)}{t} \]
    provided the limit exists.
\end{definition}

\begin{definition}[Gradient]
    The gradient of $f$ at $\x$, denoted $\grad f(\x)$, is the \emph{unique} vector in $\V$ such that the directional derivative can always be expressed as an inner product with $\grad f(\x)$:
    \[ D_{\h}f(\x) = \inner{\grad f(\x)}{\h} \quad \text{for all } \h \in \V \]
\end{definition}
\begin{remark}
    The existence and uniqueness of such a vector $\grad f(\x)$ for differentiable functions on finite-dimensional inner product spaces is guaranteed (related to the Riesz Representation Theorem applied to the derivative, which is a linear functional). Our task is to find an explicit expression for this vector.
\end{remark}

\section{The Core Calculation: Finding the Directional Derivative}

Let's compute $D_{\h}f(\x)$ for $f(\x) = \inner{\x}{\T\x}$.

\begin{enumerate}
    \item \textbf{Expand $f(\x + t\h)$:}
        We substitute $\x + t\h$ into the definition of $f$:
        \begin{align*}
            f(\x + t\h) &= \inner{\x + t\h}{\T(\x + t\h)} \\
            &= \inner{\x + t\h}{\T\x + t\T\h} \quad &\text{(Linearity of } \T\text{)} \\
            &= \inner{\x}{\T\x + t\T\h} + \inner{t\h}{\T\x + t\T\h} \quad &\text{(Linearity in 1st arg)} \\
            &= \inner{\x}{\T\x} + \inner{\x}{t\T\h} + \inner{t\h}{\T\x} + \inner{t\h}{t\T\h} \quad &\text{(Linearity in 2nd arg)} \\
            &= \inner{\x}{\T\x} + t\inner{\x}{\T\h} + t\inner{\h}{\T\x} + t^2\inner{\h}{\T\h} \quad &\text{(Linearity of inner product)} \\
            &= f(\x) + t\big( \inner{\x}{\T\h} + \inner{\h}{\T\x} \big) + t^2\inner{\h}{\T\h}
        \end{align*}

    \item \textbf{Form the difference quotient and take the limit:}
        \begin{align*}
            D_{\h}f(\x) &= \lim_{t \to 0} \frac{f(\x + t\h) - f(\x)}{t} \\
            &= \lim_{t \to 0} \frac{\big[ f(\x) + t\big( \inner{\x}{\T\h} + \inner{\h}{\T\x} \big) + t^2\inner{\h}{\T\h} \big] - f(\x)}{t} \\
            &= \lim_{t \to 0} \frac{t\big( \inner{\x}{\T\h} + \inner{\h}{\T\x} \big) + t^2\inner{\h}{\T\h}}{t} \\
            &= \lim_{t \to 0} \big[ \inner{\x}{\T\h} + \inner{\h}{\T\x} + t\inner{\h}{\T\h} \big] \\
            &= \inner{\x}{\T\h} + \inner{\h}{\T\x}
        \end{align*}
\end{enumerate}
So, we've found the directional derivative: $D_{\h}f(\x) = \inner{\x}{\T\h} + \inner{\h}{\T\x}$.

\section{The Adjoint Operator's Crucial Role}

Our result for $D_{\h}f(\x)$ isn't quite in the form $\inner{\text{something}}{\h}$ yet. We need to manipulate the terms so that $\h$ consistently appears as the second argument in the inner product. This is where the adjoint operator comes in.

\begin{definition}[Adjoint Operator]
    For a linear operator $\T: \V \to \V$ on an inner product space $\V$, its adjoint $\Tadj: \V \to \V$ is the unique linear operator satisfying:
    \[ \inner{\uvec}{\T\vvec} = \inner{\Tadj\uvec}{\vvec} \quad \text{for all } \uvec, \vvec \in \V \]
\end{definition}
\begin{remark}
    Think of the adjoint as the operator that allows $\T$ to "switch sides" within the inner product. In $\R^n$ with the standard dot product, if $\T$ is represented by matrix $A$, then $\Tadj$ is represented by the transpose $A^T$.
\end{remark}

Let's apply this to our directional derivative terms:
\begin{itemize}
    \item The first term is $\inner{\x}{\T\h}$. Using the definition of the adjoint (with $\uvec=\x, \vvec=\h$), we get:
    \[ \inner{\x}{\T\h} = \inner{\Tadj\x}{\h} \]
    This is now in the desired form.

    \item The second term is $\inner{\h}{\T\x}$. We use the symmetry of the real inner product:
    \[ \inner{\h}{\T\x} = \inner{\T\x}{\h} \]
    This is also now in the desired form.
\end{itemize}

\section{Identifying the Gradient}

Substitute these back into the expression for $D_{\h}f(\x)$:
\begin{align*}
    D_{\h}f(\x) &= \inner{\Tadj\x}{\h} + \inner{\T\x}{\h} \\
    &= \inner{\Tadj\x + \T\x}{\h} \quad &\text{(Linearity of inner product in 1st arg)} \\
    &= \inner{(\Tadj + \T)\x}{\h} \quad &\text{(Definition of sum of operators)}
\end{align*}

We have successfully expressed the directional derivative in the form $D_{\h}f(\x) = \inner{\text{Vector}}{\h}$. By the definition of the gradient, that "Vector" must be $\grad f(\x)$.

\begin{theorem}[Gradient of a Quadratic Form]
    Let $f(\x) = \inner{\x}{\T\x}$ be a quadratic form on a real inner product space $\V$, where $\T: \V \to \V$ is a linear operator. The gradient of $f$ is given by:
    \[ \grad f(\x) = (\Tadj + \T)\x \]
\end{theorem}

\section{The Special Case: Self-Adjoint Operators}

A very common and important situation is when the operator $\T$ is self-adjoint.

\begin{definition}[Self-Adjoint Operator]
    An operator $\T$ is self-adjoint if it equals its adjoint: $\Tadj = \T$.
\end{definition}
\begin{remark}
    In $\R^n$ with the dot product, this corresponds to a symmetric matrix ($A^T = A$). Self-adjoint operators have many nice properties, such as real eigenvalues and orthogonal eigenvectors.
\end{remark}

If $\T$ is self-adjoint, our gradient formula simplifies elegantly:

\begin{corollary}[Gradient for Self-Adjoint T]
    If $\T$ is self-adjoint ($\Tadj = \T$), then the gradient of $f(\x) = \inner{\x}{\T\x}$ is:
    \[ \grad f(\x) = (\T + \T)\x = 2\T\x \]
\end{corollary}

This perfectly matches the familiar result $\pdx{(\x^T A \x)} = 2A\x$ when $A$ is symmetric.

\section{Conclusion}

By stepping back from coordinates and using the abstract language of inner products and linear operators, we derived the gradient of the quadratic form $f(\x) = \inner{\x}{\T\x}$ as $\grad f(\x) = (\Tadj + \T)\x$. This derivation highlights the fundamental role of the inner product structure (defining the gradient and the adjoint) and the linearity of the operator. It also naturally reveals why the transpose (adjoint) appears in the general formula and why it simplifies for symmetric (self-adjoint) operators. This perspective provides a robust foundation for understanding differentiation in more complex vector spaces encountered in advanced mathematics, physics, and engineering.

Keep exploring the connections between abstract concepts and concrete examples â€“ it's where the deepest understanding often lies!

\end{document}