\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{parskip} % Adds space between paragraphs instead of indentation

% Theorem Environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}[theorem]{Exercise}

% Custom command for expectation and covariance
\newcommand{\E}{\mathbb{E}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}

% Math Operators
\DeclareMathOperator{\Tr}{Tr} % Trace

% Bold vectors (optional, standard math italic is also fine)
% \renewcommand{\vec}[1]{\mathbf{#1}}

\title{Lecture Notes: Random Matrices and Vectors \\ Expectations and Covariance}
\author{Advanced Regression Analysis Course}
\date{Based on Lecture Transcript}

\begin{document}
\maketitle

%----------------------------------------------------------------------
\section*{Administrative Announcements}
%----------------------------------------------------------------------

\fbox{\begin{minipage}{0.95\textwidth}
\vspace{1ex} % Add a little vertical space inside the box
Hello everyone, I hope you all had a restful break! Please take note of the following announcements:

\begin{itemize}
    \item \textbf{Midterm Quiz Date:} The midterm quiz has been scheduled for \textbf{Monday, May 12th}.
    \item \textbf{Timing:} The quiz will take place during our regular class time (2 hours). It will replace the lecture for that day.
    \item \textbf{Content Coverage:} The quiz will cover material presented up to approximately one week before the quiz date. We will provide a precise list of topics closer to the date.
    \item \textbf{Quiz Structure:} More details about the format and structure of the quiz will be announced soon.
    \item \textbf{Practice Materials:} Examples and past quiz questions from previous years will be uploaded to Moodle to help you prepare and practice.
    \item \textbf{Lecture Notes:} I aim to upload updated lecture summaries for each week at the beginning of that week. However, I often make small corrections, updates, or improvements while preparing the actual lecture.
        \begin{itemize}
            \item \textbf{Recommendation:} Please check for the latest version of the notes shortly before the lecture.
            \item \textbf{Binding Material:} As always, the definitive material for the course is what is presented on the board during the lecture.
            \item \textbf{Error Reporting:} If you spot any errors or typos in the posted notes, please let me know! I appreciate your help in keeping them accurate.
            \item \textbf{Location:} Updated notes for the current week will be in the weekly folder on Moodle, and the complete, cumulatively updated notes are available in the general course folder.
        \end{itemize}
\end{itemize}
\vspace{1ex} % Add a little vertical space inside the box
\end{minipage}}

\vspace{2ex} % Space after the announcement box

%----------------------------------------------------------------------
\section{Introduction and Motivation}
%----------------------------------------------------------------------

We are embarking on a foundational segment that builds the algebraic tools necessary for a deeper understanding of statistical models, particularly the linear model we introduced earlier. Recall that the linear model posits a relationship where the expected value of a response variable $Y$, given predictors $X = (X_1, \dots, X_p)$, is a linear function of those predictors (including an intercept term):
\[ \E[Y | X] = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p \]
Furthermore, we often make assumptions about the errors, $\epsilon_i = Y_i - \E[Y_i | X_i]$. For instance, we might assume:
\begin{itemize}
    \item $\Cov(\epsilon_i, \epsilon_j) = 0$ for $i \neq j$ (uncorrelated errors).
    \item $\Var(\epsilon_i) = \sigma^2$ for all $i$ (constant error variance, or homoscedasticity).
\end{itemize}
Working with multiple observations $(Y_1, X_1), \dots, (Y_n, X_n)$ naturally leads to collections of variables. To analyze such models efficiently and elegantly, especially when deriving properties of estimators or test statistics, it's incredibly useful to employ matrix notation. This chapter focuses on developing the "algebra" of expectations and covariances for random vectors and matrices, which will provide the language and machinery needed for our subsequent statistical developments.

%----------------------------------------------------------------------
\section{Random Matrices and Expectation}
%----------------------------------------------------------------------

\begin{definition}[Random Matrix]
A \textbf{random matrix} $Z$ of size $n \times m$ is simply an $n \times m$ matrix whose elements, $Z_{ij}$, are random variables defined on some common underlying probability space.
\[ Z = \begin{pmatrix} Z_{11} & Z_{12} & \dots & Z_{1m} \\ Z_{21} & Z_{22} & \dots & Z_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ Z_{n1} & Z_{n2} & \dots & Z_{nm} \end{pmatrix} \]
The elements $Z_{ij}$ (for $1 \le i \le n, 1 \le j \le m$) possess a \textbf{joint probability distribution}. This means their behavior is potentially interconnected; they are generally *not* assumed to be independent unless explicitly stated.
\end{definition}

\begin{remark}
The only difference between a random matrix and a standard matrix of constants is that the entries here are random variables rather than fixed numbers.
\end{remark}

\begin{definition}[Random Vector]
A particularly important special case occurs when $m=1$. An $n \times 1$ random matrix is called a \textbf{random vector} (of dimension $n$). We typically denote it as:
\[ Z = \begin{pmatrix} Z_1 \\ Z_2 \\ \vdots \\ Z_n \end{pmatrix} \]
Note that we usually omit the second index (which is always 1) for vectors. Unless otherwise specified, vectors are assumed to be \textbf{column vectors}. A random vector $Z$ resides in $\mathbb{R}^n$.
\end{definition}

\begin{definition}[Expectation of a Random Matrix]
The \textbf{expectation} (or expected value) of an $n \times m$ random matrix $Z$, denoted $\E[Z]$, is the $n \times m$ matrix obtained by taking the expectation of each element individually, provided each expectation exists.
\[ \E[Z] = \begin{pmatrix} \E[Z_{11}] & \E[Z_{12}] & \dots & \E[Z_{1m}] \\ \E[Z_{21}] & \E[Z_{22}] & \dots & \E[Z_{2m}] \\ \vdots & \vdots & \ddots & \vdots \\ \E[Z_{n1}] & \E[Z_{n2}] & \dots & \E[Z_{nm}] \end{pmatrix} \]
In shorthand, the $(i,j)$-th element of $\E[Z]$ is $(\E[Z])_{ij} = \E[Z_{ij}]$.
\end{definition}

\begin{remark}
Note that $\E[Z]$ is a \textbf{constant} matrix (a matrix of fixed numbers), not a random matrix, assuming the expectations exist.
\end{remark}

\begin{example}[Height and Weight Vector]
Consider the case where $n=2$ and $m=1$. Let $Z_1$ be the height of a randomly selected person and $Z_2$ be their weight. Then $Z = \begin{pmatrix} Z_1 \\ Z_2 \end{pmatrix}$ is a random vector in $\mathbb{R}^2$.
\begin{itemize}
    \item $Z_1$ itself is a random variable with some distribution (e.g., perhaps approximately normal).
    \item $Z_2$ itself is a random variable with some distribution.
    \item Crucially, $Z_1$ and $Z_2$ have a \textit{joint distribution}. When we sample a person, we observe *both* their height and weight simultaneously. This joint behavior is described by the distribution of the vector $Z$.
    \item Height and weight are typically *not* independent. Knowing someone's height gives us information about their likely weight. This dependence is captured in the joint distribution.
\end{itemize}
This simple example illustrates the core idea: random vectors (and matrices) group together multiple random variables whose values may be related. Our goal in regression, fundamentally, is often to understand and model such relationships.
\end{example}

\begin{remark}[Joint Distributions]
The concept of a joint distribution for the $n \times m$ elements of a random matrix $Z$ is a direct generalization of the joint distribution for two random variables $(X, Y)$, or for the $n$ components of a random vector $(Z_1, \dots, Z_n)$. It describes the probability of the variables simultaneously taking values in certain ranges.
\end{remark}

%----------------------------------------------------------------------
\section{Properties of Expectation for Random Matrices}
%----------------------------------------------------------------------

The expectation operator for random matrices inherits the linearity property from scalar expectations. Let $Z$ and $W$ be random matrices of the same size ($n \times m$), and let $A$ and $B$ be constant matrices (not random) of appropriate sizes such that the matrix multiplications below are defined.

\begin{theorem}[Linearity of Expectation]
\begin{enumerate}
    \item \textbf{Additivity:} $\E[Z + W] = \E[Z] + \E[W]$.
    \item \textbf{Multiplication by Constants:} $\E[A Z B] = A (\E[Z]) B$.
    \item \textbf{Affine Transformation:} If $C$ is a constant matrix of the same size as $Z$, then $\E[Z + C] = \E[Z] + C$. More generally, $\E[A Z + C] = A \E[Z] + C$ (assuming dimensions match for $A Z$).
\end{enumerate}
\end{theorem}

\begin{proof}
We prove these properties by examining the $(i,j)$-th element.

1.  \textbf{Additivity:}
    Let $Z$ and $W$ be $n \times m$ random matrices. The $(i,j)$-th element of $Z+W$ is $(Z+W)_{ij} = Z_{ij} + W_{ij}$. This is a sum of two scalar random variables.
    \begin{align*}
        (\E[Z + W])_{ij} &= \E[(Z + W)_{ij}] \quad &&\text{(by definition of matrix expectation)} \\
                         &= \E[Z_{ij} + W_{ij}] \quad &&\text{(by definition of matrix addition)} \\
                         &= \E[Z_{ij}] + \E[W_{ij}] \quad &&\text{(by linearity of scalar expectation)} \\
                         &= (\E[Z])_{ij} + (\E[W])_{ij} \quad &&\text{(by definition of matrix expectation)} \\
                         &= (\E[Z] + \E[W])_{ij} \quad &&\text{(by definition of matrix addition)}
    \end{align*}
    Since this holds for all $(i,j)$, we have $\E[Z + W] = \E[Z] + \E[W]$.

2.  \textbf{Multiplication by Constants:} It suffices to show (a) $\E[AZ] = A\E[Z]$ and (b) $\E[ZB] = \E[Z]B$. The general result $\E[AZB] = A\E[Z]B$ follows by applying these two sequentially (e.g., $\E[AZB] = \E[A(ZB)] = A\E[ZB] = A(\E[Z]B) = A\E[Z]B$).

    Let's prove (a) $\E[AZ] = A\E[Z]$. Assume $A$ is $p \times n$ and $Z$ is $n \times m$. Then $AZ$ is $p \times m$.
    \begin{align*}
        (\E[AZ])_{ij} &= \E[(AZ)_{ij}] \quad &&\text{(def. matrix expectation)} \\
                      &= \E\left[ \sum_{k=1}^n A_{ik} Z_{kj} \right] \quad &&\text{(def. matrix multiplication)} \\
                      &= \sum_{k=1}^n \E[A_{ik} Z_{kj}] \quad &&\text{(linearity of scalar expectation)} \\
                      &= \sum_{k=1}^n A_{ik} \E[Z_{kj}] \quad &&\text{(since $A_{ik}$ are constants)} \\
                      &= \sum_{k=1}^n A_{ik} (\E[Z])_{kj} \quad &&\text{(def. matrix expectation)} \\
                      &= (A (\E[Z]))_{ij} \quad &&\text{(def. matrix multiplication)}
    \end{align*}
    Since this holds for all $(i,j)$, we have $\E[AZ] = A\E[Z]$.

    The proof for (b) $\E[ZB] = \E[Z]B$ is analogous and left as an exercise. Assume $Z$ is $n \times m$ and $B$ is $m \times q$. Show that $(\E[ZB])_{ij} = (\E[Z]B)_{ij}$.

3.  \textbf{Affine Transformation:} Let $C$ be a constant $n \times m$ matrix.
    \begin{align*}
        \E[Z + C] &= \E[Z] + \E[C] \quad &&\text{(by Property 1)} \\
                   &= \E[Z] + C \quad &&\text{(since $\E[C_{ij}] = C_{ij}$ for constants)}
    \end{align*}
    The more general form $\E[AZ + C] = A \E[Z] + C$ follows similarly using Property 1 and Property 2(a).
\end{proof}

\begin{remark}
These properties confirm that expectation behaves linearly even when applied to matrices and vectors, which is extremely convenient for algebraic manipulations.
\end{remark}

%----------------------------------------------------------------------
\section{Covariance Matrices}
%----------------------------------------------------------------------

Just as expectation generalizes to matrices, so does the concept of variance and covariance.

\subsection{Review: Covariance for Scalar Random Variables}

Recall for scalar random variables $Z$ and $W$ with means $\mu_Z = \E[Z]$ and $\mu_W = \E[W]$:
\begin{itemize}
    \item The \textbf{covariance} is $\Cov(Z, W) = \E[(Z - \mu_Z)(W - \mu_W)]$.
    \item A useful computational formula is $\Cov(Z, W) = \E[ZW] - \mu_Z \mu_W = \E[ZW] - \E[Z]\E[W]$.
        \begin{proof}[Derivation of formula]
        $\E[(Z - \mu_Z)(W - \mu_W)] = \E[ZW - Z\mu_W - \mu_Z W + \mu_Z \mu_W]$
        $= \E[ZW] - \E[Z\mu_W] - \E[\mu_Z W] + \E[\mu_Z \mu_W]$ (by linearity)
        $= \E[ZW] - \mu_W \E[Z] - \mu_Z \E[W] + \mu_Z \mu_W$ (constants factor out)
        $= \E[ZW] - \mu_W \mu_Z - \mu_Z \mu_W + \mu_Z \mu_W = \E[ZW] - \mu_Z \mu_W$.
        \end{proof}
    \item The \textbf{variance} is a special case: $\Var(Z) = \Cov(Z, Z) = \E[(Z - \mu_Z)^2] = \E[Z^2] - (\E[Z])^2$.
    \item Covariance is \textbf{symmetric}: $\Cov(Z, W) = \Cov(W, Z)$.
    \item Covariance is \textbf{bilinear}: It is linear in each argument when the other is held fixed. For constants $a, b$ and random variables $X, Y, Z$:
        \begin{itemize}
            \item $\Cov(aX + bY, Z) = a \Cov(X, Z) + b \Cov(Y, Z)$
            \item $\Cov(X, aY + bZ) = a \Cov(X, Y) + b \Cov(X, Z)$
        \end{itemize}
        (The second follows from the first property and symmetry). Also note $\Cov(X + c, Y) = \Cov(X, Y)$ if $c$ is a constant.
\end{itemize}

\subsection{The Covariance Matrix for Random Vectors}

Now we generalize to random vectors. Let $Z$ be an $n \times 1$ random vector with mean vector $\mu_Z = \E[Z]$ (which is $n \times 1$), and let $W$ be an $m \times 1$ random vector with mean vector $\mu_W = \E[W]$ (which is $m \times 1$). Both $Z$ and $W$ are defined on the same probability space.

\begin{definition}[Cross-Covariance Matrix]
The \textbf{covariance matrix} (or \textbf{cross-covariance matrix}) between $Z$ and $W$ is the $n \times m$ matrix defined as:
\[ \Cov(Z, W) = \E[ (Z - \mu_Z) (W - \mu_W)^T ] \]
\end{definition}

\begin{remark}[Understanding the Definition]
\begin{itemize}
    \item $Z - \mu_Z$ is an $n \times 1$ random vector (mean zero).
    \item $W - \mu_W$ is an $m \times 1$ random vector (mean zero).
    \item $(W - \mu_W)^T$ is its transpose, a $1 \times m$ random row vector.
    \item The product $(Z - \mu_Z) (W - \mu_W)^T$ is an \textbf{outer product}. It results in an $n \times m$ \textbf{random matrix}.
    \[ \underbrace{(Z - \mu_Z)}_{n \times 1} \underbrace{(W - \mu_W)^T}_{1 \times m} =
       \begin{pmatrix} Z_1 - \mu_{Z_1} \\ \vdots \\ Z_n - \mu_{Z_n} \end{pmatrix}
       \begin{pmatrix} W_1 - \mu_{W_1} & \dots & W_m - \mu_{W_m} \end{pmatrix}
    \]
    The $(i,j)$-th element of this random matrix is $(Z_i - \mu_{Z_i})(W_j - \mu_{W_j})$.
    \item Taking the expectation $\E[\dots]$ applies element-wise to this $n \times m$ random matrix.
    \item Therefore, the $(i,j)$-th element of the resulting (constant) matrix $\Cov(Z, W)$ is:
    \[ (\Cov(Z, W))_{ij} = \E[ (Z_i - \mu_{Z_i})(W_j - \mu_{W_j}) ] = \Cov(Z_i, W_j) \]
    This gives a very intuitive understanding: the covariance matrix $\Cov(Z, W)$ simply collects all the pairwise scalar covariances between the components of $Z$ and the components of $W$.
    \item The resulting matrix $\Cov(Z, W)$ is an $n \times m$ \textbf{constant} matrix.
\end{itemize}
\end{remark}

\begin{definition}[Variance-Covariance Matrix]
A crucial special case arises when we consider the covariance of a random vector $Z$ with itself ($W=Z$, so $m=n$). This is called the \textbf{variance-covariance matrix} (or often just the \textbf{covariance matrix}) of $Z$.
\[ \Cov(Z) := \Cov(Z, Z) = \E[ (Z - \mu_Z) (Z - \mu_Z)^T ] \]
This is an $n \times n$ matrix.
\begin{itemize}
    \item The $(i,j)$-th element (for $i \neq j$) is $(\Cov(Z))_{ij} = \Cov(Z_i, Z_j)$.
    \item The $(i,i)$-th (diagonal) element is $(\Cov(Z))_{ii} = \Cov(Z_i, Z_i) = \Var(Z_i)$.
\end{itemize}
So, the covariance matrix $\Cov(Z)$ has the variances of the components of $Z$ on its diagonal and the covariances between pairs of components off the diagonal.
\[ \Cov(Z) = \begin{pmatrix}
\Var(Z_1) & \Cov(Z_1, Z_2) & \dots & \Cov(Z_1, Z_n) \\
\Cov(Z_2, Z_1) & \Var(Z_2) & \dots & \Cov(Z_2, Z_n) \\
\vdots & \vdots & \ddots & \vdots \\
\Cov(Z_n, Z_1) & \Cov(Z_n, Z_2) & \dots & \Var(Z_n)
\end{pmatrix} \]
Since $\Cov(Z_i, Z_j) = \Cov(Z_j, Z_i)$, the matrix $\Cov(Z)$ is always \textbf{symmetric}.
\end{definition}

\begin{remark}[Cov(Z, W) vs Cov(W, Z)]
Note that $\Cov(Z, W)$ is $n \times m$, while $\Cov(W, Z)$ is $m \times n$. They are generally not equal, but they are transposes of each other (see Property 1 below). $\Cov(Z, W)$ relates components of $Z$ to components of $W$. $\Cov(Z)$ relates components of $Z$ among themselves.
\end{remark}

\subsection{Computational Formula for Covariance Matrix}

Similar to the scalar case, there's a useful formula analogous to $\E[ZW] - \E[Z]\E[W]$.

\begin{theorem}[Computational Formula for Covariance Matrix]
For random vectors $Z$ ($n \times 1$) and $W$ ($m \times 1$),
\[ \Cov(Z, W) = \E[Z W^T] - \mu_Z \mu_W^T = \E[Z W^T] - (\E[Z])(\E[W])^T \]
\end{theorem}

\begin{proof}
We start from the definition and expand, using linearity of expectation for matrices.
\begin{align*}
\Cov(Z, W) &= \E[ (Z - \mu_Z) (W - \mu_W)^T ] \\
&= \E[ (Z - \mu_Z) (W^T - \mu_W^T) ] \quad &&\text{(transpose property)} \\
&= \E[ Z W^T - Z \mu_W^T - \mu_Z W^T + \mu_Z \mu_W^T ] \quad &&\text{(matrix distributivity)} \\
&= \E[Z W^T] - \E[Z \mu_W^T] - \E[\mu_Z W^T] + \E[\mu_Z \mu_W^T] \quad &&\text{(linearity of } \E)
\end{align*}
Now consider the terms involving constant vectors/matrices $\mu_Z, \mu_W^T$:
\begin{itemize}
    \item $\E[Z \mu_W^T] = (\E[Z]) \mu_W^T = \mu_Z \mu_W^T$ (since $\mu_W^T$ is constant)
    \item $\E[\mu_Z W^T] = \mu_Z (\E[W^T])$ (since $\mu_Z$ is constant)
    \item $\E[\mu_Z \mu_W^T] = \mu_Z \mu_W^T$ (since $\mu_Z \mu_W^T$ is constant)
\end{itemize}
We also need the property that $\E[W^T] = (\E[W])^T = \mu_W^T$.
\begin{proof}
\textit{Proof that $\E[W^T] = (\E[W])^T$:}
We have $W^T = \begin{pmatrix} W_1 & W_2 & \dots & W_m \end{pmatrix}$.
Then $\E[W^T] = \begin{pmatrix} \E[W_1] & \E[W_2] & \dots & \E[W_m] \end{pmatrix}$.
Also, $\E[W] = \begin{pmatrix} \E[W_1] \\ \vdots \\ \E[W_m] \end{pmatrix}$, so $(\E[W])^T = \begin{pmatrix} \E[W_1] & \dots & \E[W_m] \end{pmatrix}$.
Comparing the results, we see that $\E[W^T] = (\E[W])^T$.
\end{proof}
Substituting back:
\begin{align*}
\Cov(Z, W) &= \E[Z W^T] - \mu_Z \mu_W^T - \mu_Z (\E[W^T]) + \mu_Z \mu_W^T \\
&= \E[Z W^T] - \mu_Z \mu_W^T - \mu_Z \mu_W^T + \mu_Z \mu_W^T \\
&= \E[Z W^T] - \mu_Z \mu_W^T
\end{align*}
This formula expresses the $n \times m$ covariance matrix in terms of the expectation of the $n \times m$ outer product $Z W^T$ and the $n \times m$ outer product of the mean vectors $\mu_Z \mu_W^T$.
\end{proof}

%----------------------------------------------------------------------
\section{Properties of Covariance Matrices}
%----------------------------------------------------------------------

Covariance matrices obey several important algebraic properties, extending the bilinearity of scalar covariance. Let $Z, W, R$ be random vectors, and $A, B$ be constant matrices, and $a$ be a constant vector, all with compatible dimensions. Let $\mu_Z = \E[Z]$, etc.

\begin{theorem}[Properties of Covariance Matrices]
\begin{enumerate}
    \item \textbf{Transpose Property:} $\Cov(Z, W) = (\Cov(W, Z))^T$.
    \item \textbf{Additivity (in first argument):} $\Cov(Z + R, W) = \Cov(Z, W) + \Cov(R, W)$. (Assumes $Z, R$ have same dimension).
    \item \textbf{Linear Transformations:} $\Cov(A Z, B W) = A \Cov(Z, W) B^T$.
    \item \textbf{Variance of Linear Transformation:} As a special case of (3) with $W=Z$ and $B=A$, $\Cov(A Z) = A \Cov(Z) A^T$.
    \item \textbf{Variance of a Linear Combination:} For a constant vector $a$, $a^T Z$ is a scalar random variable, and its variance is $\Var(a^T Z) = a^T \Cov(Z) a$.
    \item \textbf{Positive Semi-Definiteness:} The variance-covariance matrix $\Cov(Z)$ is always symmetric and positive semi-definite (PSD).
\end{enumerate}
\end{theorem}

\begin{proof}
1.  \textbf{Transpose Property:}
    \begin{align*}
    \Cov(W, Z) &= \E[ (W - \mu_W) (Z - \mu_Z)^T ] \\
    (\Cov(W, Z))^T &= (\E[ (W - \mu_W) (Z - \mu_Z)^T ])^T \\
    &= \E[ ((W - \mu_W) (Z - \mu_Z)^T)^T ] \quad &&\text{(using } (\E[M])^T = \E[M^T]) \\
    &= \E[ ( (Z - \mu_Z)^T )^T (W - \mu_W)^T ] \quad &&\text{(using } (XY)^T = Y^T X^T) \\
    &= \E[ (Z - \mu_Z) (W - \mu_W)^T ] \quad &&\text{(using } (M^T)^T = M) \\
    &= \Cov(Z, W)
    \end{align*}

2.  \textbf{Additivity:} We show this element-wise. Let $Z, R$ be $n \times 1$ and $W$ be $m \times 1$.
    \begin{align*}
    (\Cov(Z+R, W))_{ij} &= \Cov( (Z+R)_i, W_j ) \quad &&\text{(def. cov matrix element)} \\
    &= \Cov( Z_i + R_i, W_j ) \quad &&\text{(def. vector addition)} \\
    &= \Cov( Z_i, W_j ) + \Cov( R_i, W_j ) \quad &&\text{(linearity of scalar cov)} \\
    &= (\Cov(Z, W))_{ij} + (\Cov(R, W))_{ij} \quad &&\text{(def. cov matrix element)} \\
    &= (\Cov(Z, W) + \Cov(R, W))_{ij} \quad &&\text{(def. matrix addition)}
    \end{align*}
    Since this holds for all $(i,j)$, the matrix identity follows. (Linearity in the second argument follows using Property 1).

3.  \textbf{Linear Transformations:} Let $Z$ be $n \times 1$, $W$ be $m \times 1$, $A$ be $p \times n$, $B$ be $q \times m$. Then $AZ$ is $p \times 1$ and $BW$ is $q \times 1$. $\Cov(AZ, BW)$ will be $p \times q$.
    We use the definition and linearity of expectation. Note $\E[AZ] = A\mu_Z$ and $\E[BW] = B\mu_W$.
    \begin{align*}
    \Cov(AZ, BW) &= \E[ (AZ - \E[AZ]) (BW - \E[BW])^T ] \\
    &= \E[ (AZ - A\mu_Z) (BW - B\mu_W)^T ] \\
    &= \E[ A(Z - \mu_Z) (B(W - \mu_W))^T ] \\
    &= \E[ A(Z - \mu_Z) (W - \mu_W)^T B^T ] \quad &&\text{(transpose property }(XY)^T = Y^T X^T) \\
    &= A \left( \E[ (Z - \mu_Z) (W - \mu_W)^T ] \right) B^T \quad &&\text{(linearity } \E[CXD]=C\E[X]D) \\
    &= A \Cov(Z, W) B^T
    \end{align*}
    (The element-wise proof presented in the lecture involves manipulating sums $\sum_k A_{ik} Z_k$ and $\sum_l B_{jl} W_l$ within the scalar covariance definition $\Cov((\dots)_i, (\dots)_j)$, applying bilinearity, and then carefully recognizing the resulting double summation $\sum_k \sum_l A_{ik} \Cov(Z_k, W_l) B_{jl}$ as the $(i,j)$-th element of the matrix product $A \Cov(Z, W) B^T$. While more tedious, it reinforces the element-wise definitions.)

4.  \textbf{Variance of Linear Transformation:} This follows directly from Property 3 by setting $W = Z$ and $B = A$.
    \[ \Cov(AZ) = \Cov(AZ, AZ) = A \Cov(Z, Z) A^T = A \Cov(Z) A^T \]

5.  \textbf{Variance of Linear Combination:} Let $a$ be an $n \times 1$ constant vector. Then $a^T Z$ is a $1 \times 1$ matrix (a scalar). Applying Property 4 with $A = a^T$:
    \[ \Cov(a^T Z) = a^T \Cov(Z) (a^T)^T = a^T \Cov(Z) a \]
    Since variance of any (scalar) random variable $Y = a^T Z$ must be non-negative ($\Var(Y) \ge 0$), it follows that $x^T \Cov(Z) x \ge 0$ for all vectors $x$ (of compatible dimension). This is the definition of $\Cov(Z)$ being positive semi-definite. We already noted $\Cov(Z)$ is symmetric.
\end{proof}

\begin{exercise}
Prove Property 3, $\Cov(A Z, B W) = A \Cov(Z, W) B^T$, using the computational formula $\Cov(X, Y) = \E[XY^T] - \E[X](\E[Y])^T$.
(Hint: Apply the formula with $X=AZ$ and $Y=BW$, use linearity of $\E$, and properties of transpose).
\end{exercise}

%----------------------------------------------------------------------
\section{Concluding Remarks}
%----------------------------------------------------------------------

We have established the basic definitions and properties of expectation and covariance for random vectors and matrices. These tools, particularly the linearity of expectation and the transformation properties of covariance matrices, are essential for manipulating and understanding the statistical models we will work with, especially the multivariate linear model. They provide a compact and powerful notation for expressing complex relationships and derivations. We will apply these concepts extensively in the upcoming lectures.

\end{document}